{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing MLFlow\n",
    "#Commenting out to not run it again\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "#    y = np.array([0, 0, 1, 1, 1, 0])\n",
    "#    lr = LogisticRegression()\n",
    "#    lr.fit(X, y)\n",
    "#    score = lr.score(X, y)\n",
    "#    print(\"Score: %s\" % score)\n",
    "#    mlflow.log_metric(\"score\", score)\n",
    "#    mlflow.sklearn.log_model(lr, \"model\")\n",
    "#    print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)\n",
    "#    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the experiment on mlflow\n",
    "experiment_name = \"RF 100 Games\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csvs/Games_1_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X and y variables\n",
    "X = df.drop(['game_id','home_win'], axis=1)\n",
    "y = df['home_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train2 = sc.fit_transform(x_train)\n",
    "x_test2 = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script for Logistical Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "for name,method in [('RandomForestClassifier', RandomForestClassifier(n_estimators = 10, criterion = 'entropy',random_state=10))]: \n",
    "    method.fit(x_train2,y_train)\n",
    "    predict = method.predict(x_test2)\n",
    "    target_names=['loss', 'win']\n",
    "    print(confusion_matrix(y_test,predict))  \n",
    "    print(classification_report(y_test,predict,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2 - Testing different accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping game_id\n",
    "df.drop('game_id', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X and y variables\n",
    "X = df.drop(['home_win'], axis=1)\n",
    "y = df['home_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script for Logistical Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "for name,method in [('RandomForestClassifier', RandomForestClassifier(n_estimators = 10, criterion = 'entropy',random_state=10))]: \n",
    "    method.fit(x_train,y_train)\n",
    "    predict = method.predict(x_test)\n",
    "    target_names=['loss', 'win']\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predict - y_test)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(method, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    n2_scores = cross_val_score(method, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('MAE: %.3f (%.3f)' % (mean(n2_scores), std(n2_scores)))\n",
    "    #Accuracy performance\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "    # Print out the mean absolute error (mae)\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "    print(accuracy_score(y_test, predict))\n",
    "    print(confusion_matrix(y_test,predict))  \n",
    "    print(classification_report(y_test,predict,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing RF Model with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the experiment on mlflow\n",
    "experiment_name = \"100 Games Test\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "df = pd.read_csv('Games_1_100.csv')\n",
    "df.drop('game_id', axis=1, inplace = True)\n",
    "\n",
    "#Defining X and y variables\n",
    "X = df.drop('home_win', axis=1)\n",
    "y = df['home_win']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "#Script for Logistical Regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "for name,method in [('RandomForestClassifier', RandomForestClassifier(n_estimators = 10, criterion = 'entropy',random_state=10))]: \n",
    "    method.fit(x_train2,y_train)\n",
    "    predict = method.predict(x_test2)\n",
    "    target_names=['loss', 'win']\n",
    "    metrics = {'Accuracy':accuracy_score(y_test,predict)}\n",
    "    cm = confusion_matrix(y_test, predict)\n",
    "    t_n, f_p, f_n, t_p = cm.ravel()\n",
    "    mlflow.log_metric(\"tn\", t_n)\n",
    "    mlflow.log_metric(\"fp\", f_p)\n",
    "    mlflow.log_metric(\"fn\", f_n)\n",
    "    mlflow.log_metric(\"tp\", t_p)\n",
    "    # Log in mlflow (metrics)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.sklearn.log_model(method, \"RF-model\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the experiment on mlflow\n",
    "experiment_name = \"100 Games Test\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "df = pd.read_csv('csvs/Games_1_100.csv')\n",
    "\n",
    "#Dropping game_id\n",
    "df.drop('game_id', axis=1, inplace = True)\n",
    "#Defining X and y variables\n",
    "X = df.drop('home_win', axis=1)\n",
    "y = df['home_win']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "#Script for RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "n_estimators = 10\n",
    "max_depth = 6\n",
    "max_features = 3 \n",
    "\n",
    "for name,method in [('RandomForestClassifier', RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features, random_state=10))]: \n",
    "    method.fit(x_train,y_train)\n",
    "    predict = method.predict(x_test)\n",
    "    target_names=['loss', 'win']\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_trees\", n_estimators)\n",
    "    mlflow.log_param(\"maxdepth\", max_depth)\n",
    "    mlflow.log_param(\"max_feat\", max_features)\n",
    "\n",
    "    #Creating metrics\n",
    "    metrics = {'Accuracy':accuracy_score(y_test,predict)}\n",
    "    cm = confusion_matrix(y_test, predict)\n",
    "    t_n, f_p, f_n, t_p = cm.ravel()\n",
    "\n",
    "    # Log in mlflow (metrics)\n",
    "    mlflow.log_metric(\"tn\", t_n)\n",
    "    mlflow.log_metric(\"fp\", f_p)\n",
    "    mlflow.log_metric(\"fn\", f_n)\n",
    "    mlflow.log_metric(\"tp\", t_p)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    #Logging model\n",
    "    mlflow.sklearn.log_model(method, \"RF-model\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Model on first 100 games to start\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#Launch the experiment on mlflow\n",
    "experiment_name = \"100 Games Test\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "df = pd.read_csv('csvs/Games_1_100.csv')\n",
    "\n",
    "#Dropping game_id\n",
    "df.drop('game_id', axis=1, inplace = True)\n",
    "\n",
    "#Defining X and y variables\n",
    "X = df.drop(['home_win'], axis=1)\n",
    "y = df['home_win']\n",
    "\n",
    "#splitting data for training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "#Script for Logistical Regression\n",
    "solver = 'liblinear'\n",
    "global model\n",
    "model = LogisticRegression(solver = solver, random_state=10)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "target_names=['loss', 'win']\n",
    "\n",
    "# Log parameters\n",
    "mlflow.log_param(\"solver\", solver)\n",
    "\n",
    "#Creating metrics\n",
    "metrics = {'Accuracy':accuracy_score(y_test,y_pred)}\n",
    "cm = confusion_matrix(y_test, predict)\n",
    "t_n, f_p, f_n, t_p = cm.ravel()\n",
    "\n",
    "#Log in mlflow (metrics)\n",
    "mlflow.log_metric(\"tn\", t_n)\n",
    "mlflow.log_metric(\"fp\", f_p)\n",
    "mlflow.log_metric(\"fn\", f_n)\n",
    "mlflow.log_metric(\"tp\", t_p)\n",
    "mlflow.log_metrics(metrics)\n",
    "\n",
    "#Logging model\n",
    "mlflow.sklearn.log_model(model, \"LR-model\")\n",
    "mlflow.end_run()\n",
    "\n",
    "#print(accuracy_score(y_test, y_pred))\n",
    "# Report training set score\n",
    "#train_score = model.score(x_train, y_train) * 100\n",
    "# Report test set score\n",
    "#test_score = model.score(x_test, y_test) * 100\n",
    "#print(confusion_matrix(y_test,y_pred))  \n",
    "#print(classification_report(y_test,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking into feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09078132,  0.14094186,  0.20727365,  0.47025203,  0.43889043,\n",
       "        0.90348524,  0.6187065 , -0.19073833,  0.0078641 , -0.0482597 ,\n",
       "        0.076516  , -0.64830143])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding intercept and coefficients of Log Regression\n",
    "w0 = model.intercept_[0]\n",
    "w = w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12 = model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate feature importance in Log Reg\n",
    "labels = df.drop('home_win', axis=1).columns\n",
    "feature_importance = pd.DataFrame(labels, columns = [\"feature\"])\n",
    "feature_importance[\"importance\"] = pow(math.e, w)\n",
    "feature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n",
    " \n",
    "#Image formatting\n",
    "axis_fs = 18 #fontsize\n",
    "title_fs = 22 #fontsize\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#Plotting\n",
    "ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance)\n",
    "ax.set_xlabel('Importance',fontsize = axis_fs) \n",
    "ax.set_ylabel('Feature', fontsize = axis_fs)#ylabel\n",
    "ax.set_title('Logistic Regression\\nfeature importance', fontsize = title_fs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/LR_feature_importance.png\",dpi=120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb5d0a65b500759bcde1c4c1ad0551eaece71d5bef76353acf57400c52edb49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
